---
title: "쉘로 데이터 마이닝"
teaching: 40
exercises: 20
questions:
- "파일 내부 데이터를 어떻게 찾아낼까?"
- "데이터를 어떻게 계수하는가?"
objectives:
- "쉘로 줄수, 단어갯수, 문자갯수를 세는 방법을 이해한다"
- "쉘로 파일을 마이닝하는 방법을 이해하고, 매칭된 라인을 추출하는 방법도 이해한다"
- "정규표현식과 쉘을 사용해서 마이닝 결과를 조합하는 방법을 이해한다"
keypoints:
- "`wc` 명령어는 계수하는 명령어다"
- "`wc` 명령어를 `-w`, `-l` 플래그를 사용해서 파일 혹은 일련의 파일에 포함된 단어와 줄수를 계수한다"
- "`> 하위디렉토리/파일명`과 같이 리디렉터를 사용해서 작업결과를 하위디렉토리 파일에 저정한다"
- "`grep` 명령어를 사용해서 파일 내부 문자열 인스턴스를 찾아낸다"
- "`grep` 명령어와 `-c` 플래그를 사용해서 문자열 인스턴스 갯수를 계수하고, `-i` 플래그를 사용하면 대소문자를 구별하지 않고 계수하고, `-v` 플래그는 결과에서 특정 문자열을 제외하고, `-w` 플래그는 단어 자체 검색결과를 반환한다"
- "`--file=list.txt` 표현식을 사용해서 `list.txt` 파일을 질의문에 사용되는 문자열 원천이라는 점을 명시한다"
- "유닉스 쉘을 사용하면 연구 데이터와 연구 프로젝트를 단순히 계수하고 마이닝하는 방법으로, 상기 명령어와 플래그를 조합해서 복잡한 질의문을 생성한다"

---

##  연구 데이터를 조작하고, 계수하고, 마이닝하기

이제 유닉스 쉘로 작업할 준비가 되어, 데이터를 계수하고 마이닝하는 방법을 학습해보자.
오히려 단순해서 현재 본인 작업을 혁신적으로 완전히 바꿀 것 같지는 않다.
하지만, 지난주에 다룬 일관된 파일구조와 파일명과 이어진다.
일관된 파일구조와 파일명은 작업 데이터를 계수하고 마이닝하는데 사용되는 
강력한 명령어 집합의 토대가 된다.

## 계수(Counting)

유닉스 쉘을 사용해서 파일에 든 내용물을 계수하는 것으로 시작해보자.
유닉스 쉘을 사용하면, 여기 저기 흩어져 저장된 파일에서 재빠른 집계가 가능하다.
이는 표준 오피스 제품에 덮혀있는 그래픽 사용자 인터페이스를 사용하면 달성하기 쉽지 않는 과제다.

유닉스쉘에서 `cd` 명령어를 사용해서 작업 데이터가 담긴 디렉토리로 탐험해 나간다.

~~~
$ cd data
~~~
{: .bash}

언제든지 현재 작업하고 있는 디렉토리를 확인하고자 할 때, `pwd` 를 타이핑하고 엔터키를 친다.

~~~
$ pwd
~~~
{: .bash}
~~~
/Users/riley/Desktop/data
~~~
{: .output}

`ls -lh` 명령어를 타이핑하고 나서 엔터키를 친다.
파일과 하위 디렉토리에 담긴 정보를 열거해서 쭉 뿌려준다.

~~~
$ ls -lh
~~~
{: .bash}
~~~
total 90120
-rwxr-xr-x  1 riley  staff   3.6M Jul 17 14:33 2014-01-31_JA-africa.tsv
-rwxr-xr-x  1 riley  staff   7.4M Jul 17 14:33 2014-01-31_JA-america.tsv
-rw-r--r--  1 riley  staff    29M Jul 17 14:33 2014-01_JA.tsv.zip
-rwxr-xr-x  1 riley  staff   1.8M Jul 17 14:33 2014-02-01_JA-art .tsv
-rwxr-xr-x  1 riley  staff   1.4M Jul 17 14:33 2014-02-02_JA-britain.tsv
-rwxr-xr-x  1 riley  staff    13B Jul 17 14:33 gallic.txt
-rwxr-xr-x  1 riley  staff   598K Jul 17 14:33 gulliver.txt
~~~
{: .output}

`data` 디렉토리에는 압축된 `2014-01_JA.tsv` 데이터셋이 담겨져 있는데 저널 논문 메타데이터, `2014-01_JA.tsv`파일에서 파생된 `.tsv` 파일 4개, 
앞선 학습에서 생성한 `gulliver.txt` , 나중에 언급할 `gallic.txt` 파일이 담겨 있따.
`2014-01_JA.tsv` 파일 'Title' 필드에 `africa` 혹은 `america` 같은 키워드가 담긴 데이터가 `.tsv` 파일 4개 각각에는 포함되어 있다.
시작하기 전에, `2014-01_JA.tsv.zip` 파일 압축을 풀어 `data/` 디렉토리에 담아둔다.

> ## TSV 파일
> TSV 파일은 데이터 각 행별 기본단위가 탭으로 구분된 파일이다.
> 탭으로 구분된다는 점에서 콤마로 구분되는 CSV(콤마 구분값) 파일과 유사하다. 
> CSV 파일이 더 일반적이지만 작업할 데이터와 같은 유형의 파일은 문제가 될 수지가 있다.
> 왜냐하면 기본단위 셀에 콤마가 포함되어 있기 때문이다.
> (적절히 인코딩하면 이런 문제는 극복될 수 있다)
> 텍스터 편집기 혹은 리브레 오피스 Calc, 마이크로소프트 엑셀 같은 스프레드쉬트 프로그램으로 
> 불러읽어들일 수 있다.
{: .callout}

`data/` 디렉토리에 담긴 파일 내용을 계수해보자.

계수에 사용되는 유닉스 명령어는 `wc`가 있다.
`wc` 명령어와 `-w` 플래그를 조합해서 컴퓨터에 계수되는 파일명과 단어빈도수를 출력하도록 명령한다.

~~~~
$ wc -w 2014-01-31_JA-africa.tsv 
~~~~
{: .bash}
~~~
511261 2014-01-31_JA-africa.tsv
~~~
{: .output}

`-w` 같은 플래그는 유닉스 쉘로부터 가장 많은 것을 뽑아내는 핵심적인 부분으로
명령어에 대한 제어권를 더 잘 행사할 수 있게 해준다.

단어 빈도수보다 행수에 대한 요청이 있는 경우 행을 계수하는 플래그를 활용한다.

~~~
$ wc -l 2014-01-31_JA-africa.tsv
~~~
{: .bash}
~~~
13712 2014-01-31_JA-africa.tsv
~~~
{: .output}

`wc` 명령어와 `-l` 플래그를 조합하면, 파일명과 더불어 행수가 출력된다.

마지막으로, 다음 명령어를 타이핑하고 엔터키를 친다.

~~~
$ wc -c 2014-01-31_JA-africa.tsv 
~~~
{: .bash}
~~~
3773660 2014-01-31_JA-africa.tsv
~~~
{: .output}

`wc` 명령어와 `-c` 플래그 조합하면 `2014-01-31_JA-africa.tsv` 파일에 포함된 문자갯수를 셀 수 있다.

**주의: 맥 OS X 사용자는 `-c` 플래그를 `-m` 으로 바꿔야 한다.**

상기 플래그 세개를 사용함으로써, `wc`를 사용하는 가장 명확한 사실은 
재빨리 디지털 형태를 갖는 다양한 저작물 형상을 비교할 수 있다는 점이다 -
예를 들어, 특정 책의 각 페이지별 단어 갯수, 여러 신문에 걸쳐 페이지별 문자 분포,
시에서 적힌 평균 단어 길이.
훨씬 더 복잡한 질의문을 생성하는데 와일드카드와 플래그를 `wc`와 조합하여 사용하는 것도 일반적이다.

> ## 파일 다수에 WC 적용
>
> `wc -l 2014-01-31_JA-a*.tsv` 명령어가 어떤 작업을 수행하는지 추축해보자.
>
> ~~~
> $ wc -l 2014-01-31_JA-a*.tsv
> ~~~
> {: .bash}
>
> > ## 해답
> > ~~~
> > 13712 2014-01-31_JA-africa.tsv
> > 27392 2014-01-31_JA-america.tsv
> > 41104 total
> > ~~~
> > {: .output}
> > `2014-01-31_JA-africa.tsv`, `2014-01-31_JA-america.tsv` 파일에 대해 행수를 세고 결과를 출력한다.
> > 따라서, 연구 데이터 두 데이터셋에 대해 간단한 비교 수단이 제공되었다.
> {: .solution}
{: .challenge}

물론, 파일이 몇개 되지 않는 경우 리브레 오피스 Calc, 마이크로소프트 엑셀, 혹은 유사 스프레드쉬트 프로그램을 사용해서 
두 문서의 행갯수를 계수하는 것이 더 빠를 수도 있다.
하지만, 수십, 수백, 수천 문서에 대해서 행갯수를 비교하는 경우, 유닉스 쉘이 명백한 장점이 속도에 있다.

더욱이, 데이터셋 갯수가 증가하는 경우, 유닉스 쉘을 활용함으로써 수작업으로 행갯수를 복사해서, 화면 출력결과를 복사해서 붙여넣기 신공을 사용하는 것보다 더 많은 작업을 수행할 수 있다.
앞서 살펴본 `>` 리디렉션 연산자를 사용해서 작업처리 결과를 새로운 파일에 내보내서 저장시킨다.

~~~
$ wc -l 2014-01-31_JA-a*.tsv > results/2016-07-19_JA-a-wc.txt
~~~
{: .bash}
~~~
-bash: results/2016-07-19_JA-a-wc.txt: No such file or directory
~~~
{: .error}

작업결과를 새로운 파일에 저장할 `results/` 디렉토리가 없다고 경고를 주는 배쉬 오류 메시지를 받았다.
해당 디렉토리를 생성해서 오류를 고쳐보자.

~~~
$ mkdir results
$ ls -F
~~~
{: .bash}
~~~
2014-01-31_JA-africa.tsv*	2014-02-02_JA-britain.tsv*
2014-01-31_JA-america.tsv*	gallic.txt*
2014-01_JA.tsv			gulliver.txt*
2014-01_JA.tsv.zip		results/
2014-02-01_JA-art .tsv*
~~~
{: .output}

OK, `results/` 디렉토리가 존재하는 것이 확인되면, `wc` 명령어를 다시 실행해보자.

~~~
$ wc -l 2014-01-31_JA-a*.tsv > results/2016-07-19_JA-a-wc.txt
~~~
{: .bash}

이전 경우와 마찬가지로 동일한 명령어를 실행하지만, 실행결과를 화면에 출력하는 대신에 
`DATE_JA_a-wc.txt` 파일에 저장시킨다.
`DATE_JA_a-wc.txt` 파일명 앞에 `results/` 디렉토리명을 붙여서,
쉘로 하여금 `results` 하위 디렉토리에 `.txt` 파일을 저장하게 만든다.
실행결과를 확인하려면, `results` 하위 디렉토리로 이동하면 확인된다:

~~~
$ cd results
$ ls 
~~~
{: .bash}
~~~
2016-07-19_JA-a-wc.txt
~~~
{: .output}

쉘에서 파일에 담긴 내용을 확인하려면 다음 명령어를 실행한다: 

~~~
$ head 2016-07-19_JA-a-wc.txt
~~~
{: .bash}
~~~
   13712 2014-01-31_JA-africa.tsv
   27392 2014-01-31_JA-america.tsv
   41104 total
~~~
{: .output}

## 마이닝(Mining)

유닉스 쉘은 파일내부 단어수, 문자수, 행수를 계수하는 것 이상을 수행할 수 있다.
`grep` 명령어(**global regular expression print**, 유닉스 텍스트 검색 명령어)를 사용해서
특정 문자열을 여기저기 흩어진 다수 파일에서 검색한다.
대다수 운영체제 혹은 오피스 프로그램에서 제공되는 그래픽 인터페이스 보다 훨씬 더 빨리 검색작업을 수행한다.
`>` 연산자와 조합하면 `grep` 명령어는 강력한 연구도구가 된다.
다수 파일에 존재하는 문자 혹은 단어 군집을 마이닝해서 작업결과를 신규 파일에 저장한다.
이런 작업흐름에서 유일한 제한점은 연구자 본인 상상력, 작업 데이터 형태, 수천 혹은 수만 파일을 작업할 경우 컴퓨팅 자원이 된다.

`grep` 명령어를 사용하기 전에, 먼저 `data` 디렉토리를 탐험해보자. (`results/` 디렉토리에서 `cd ..` 명령어를 타이핑한다)

~~~
$ grep 1999 *.tsv
~~~
{: .bash}

상기 명령어는 문자열, '1999'에 대해 해당 조건이 만족되는 해당 디렉토리에 존재하는 모든 파일(`.tsv` 파일)을 찾아낸다.
그리고 나서, 작업결과를 쉘화면에 출력한다.

가장 최근에 실행한 명령어로 되돌아 가는데 키보드 위쪽 화살표를 누른다.
`grep 1999 *.tsv` 명령어를 `grep -c 1999 *.tsv` 으로 고치고 나서 엔터키를 친다.

~~~
$ grep -c 1999 *.tsv
~~~
{: .bash}
~~~
2014-01-31_JA-africa.tsv:804
2014-01-31_JA-america.tsv:1478
2014-01_JA.tsv:28767
2014-02-01_JA-art .tsv:407
2014-02-02_JA-britain.tsv:284
~~~
{: .output}

쉘이 이제 `*.tsv` 파일 각각에 출현된 문자열 1999 빈도수를 출력한다.
이전 명령어 실행결과와 비교하면, 저널 논문에 포함된 날짜를 지칭하는 것으로 보인다.

문자열이 꼭 숫자일 필요는 없다.

~~~
$ grep -c revolution *.tsv
~~~
{: .bash}
~~~
2014-01-31_JA-africa.tsv:20
2014-01-31_JA-america.tsv:34
2014-01_JA.tsv:867
2014-02-01_JA-art .tsv:11
2014-02-02_JA-britain.tsv:9
~~~
{: .output}

상기 명령어는 정의된 파일에 포함된 문자열 `revolution` 인스턴스 갯수를 세고 나서 쉘에 출력결과를 뿌려낸다.
이제 상기 명령어를 아래와 같이 수정하고 나서 출력결과 각각이 어떻게 다른지 살펴보자:

~~~
$ grep -ci revolution *.tsv
~~~
{: .bash}
~~~
2014-01-31_JA-africa.tsv:118
2014-01-31_JA-america.tsv:1018
2014-01_JA.tsv:9327
2014-02-01_JA-art .tsv:110
2014-02-02_JA-britain.tsv:122
~~~
{: .output}

명령어를 반복실행하지만, 대소문자 구별하지 않고(`revolution` 과 `Revolution` 포함) 계수한 결과를 출력한다.
`revolution` 키워드를 포함한 저널 논문 제목이 상당히 증가된 것이 확인된다.
앞서처럼, `> results/` 명령어를 추가하고 나서, 파일명(`.txt` 형식)을 붙여 데이터 파일에 작업결과를 저장하자.

지금까지, 파일에 담긴 문자열을 계수하고 결과를 쉘로 화면에 뿌리던가 파일에 저장했다.
`grep`의 진정한 힘은 이를 사용해서 하나 혹은 다수 파일로부터 표형식의 데이터(혹은 임의 데이터)의
부분집합 데이터를 만들어 낼 수 있다는 점에서 나온다.

~~~
$ grep -i revolution *.tsv
~~~
{: .bash}

상기 명령어는 정의된 파일(`*.tsv`)을 찾아 대소문자 구별하지 않고 `revolution` 문자열을 포함한 행을 쉘 화면에 뿌린다.

~~~
$ grep -i revolution *.tsv > results/2016-07-19_JAi-revolution.tsv
~~~
{: .bash}

상기 명령어는 이렇게 부분집합으로 뽑아낸 데이터를 파일에 저장시킨다.

하지만, 저장된 파일을 살펴보변, 'revolution' 문자열이 포함된 모든 인스턴스가 담겨진다.
즉, 'revolution' 뿐만 아니라, 'revolutionary' 같은 단어도 포함된다.
생각해 보면, 아마도 원하는 바는 아니다. 고맙게도, `-w` 플래그를 사용하게 되면
해당 단어만 온전히 검색해서, 검색정도를 훨씬 더 높이게 된다.

~~~
$ grep -iw revolution *.tsv > results/DATE_JAiw-revolution.tsv
~~~
{: .bash} 

상기 스크립트는 정의된 파일에 대해 대소문자 구분하지 않고 `revolution` 단어가 온전히 포함된
행만 뽑아내서 지정된 `.tsv` 파일에 내보내고 저장시킨다.

생성한 두 파일 사이 차이를 다음 명령어를 통해 확인할 수 있다.

~~~
$ wc -l results/*.tsv
~~~
{: .bash}
~~~
   10695 2016-07-19_JAi-revolution.tsv
    7859 2016-07-19_JAw-revolution.tsv
   18554 total
~~~
{: .output}

마지막으로, **정규표현식 구문(regular expression syntax)**을 사용해서 유사 단어도 검색할 수 있다.

`gallic.txt` 파일에는 `fr[ae]nc[eh]` 문자열이 포함되어 있다.

~~~
$ cat gallic.txt
~~~
{: .bash}
~~~
fr[ae]nc[eh]
~~~
{: .output}

꺾쇠 괄호는 컴퓨터로 하여금 특정 범위에 지정된 문자를 매칭하게 만든다.
`grep` 명령어와 사용되면:

~~~
$ grep -iw --file=gallic.txt *.tsv
~~~
{: .bash}

쉘은 해당 문자열이 포함된 핵을 각각 출력시킨다:

~~~
- france
- french
- frence
- franch
~~~
{: .output}

`-o` 플래그를 포함시켜서 매칭되는 행만 출력시킨다. (결과를 격리시키거나 검증하는데 편리함)

~~~
$ grep -iwo revolution *.tsv
~~~
{: .bash}

혹은:

~~~
$ grep -iwo --file=gallic.txt *.tsv
~~~
{: .bash}

주변에 있는 사람과 짝지어서 다음 연습문제를 푸시오:

> ## 대소문자 구분 검색
> 해당 디렉토리 `.tsv` 파일에서 본인이 선택한 단어를 대소문자 구분하여 검색하시오.
> 작업실행결과를 화면에 뿌리시오.
> 
> > ## 해답
> > ~~~
> > grep hero *.tsv
> > ~~~
> > {: .bash}
> {: .solution}
{: .challenge}

> ## 선택한 파일에 대소문자 구분 검색
> 해당 디렉토리 'America', 'Africa' `.tsv` 파일에서 본인이 선택한 단어를 대소문자 구분하여 검색하시오.
> 작업실행결과를 화면에 뿌리시오.
>
> > ## 해답
> > ~~~
> > grep hero *a.tsv
> > ~~~
> > {: .bash}
> {: .solution}
{: .challenge}

> ## (대소문자 구분) 단어 갯수 세기
> 해당 디렉토리 'America', 'Africa' `.tsv` 파일에서 본인이 선택한 단어를 대소문자 구분하여 계수하시오.
> 작업실행결과를 화면에 뿌리시오.
>
> > ## 해답
> > ~~~
> > grep -c hero *a.tsv
> > ~~~
> > {: .bash}
> {: .solution}
{: .challenge}

> ## (대소문자 구분하지 않고) 단어 갯수 세기 
> 해당 디렉토리 'America', 'Africa' `.tsv` 파일에서 본인이 선택한 단어를 대소문자 구분하지 말고 계수하시오.
> 작업실행결과를 화면에 뿌리시오.
>
> > ## 해답
> > ~~~
> > grep -ci hero *a.tsv
> > ~~~
> > {: .bash}
> {: .solution}
{: .challenge}

> ## 선택한 파일에 대소문자 구분 검색
> 해당 디렉토리 'America', 'Africa' `.tsv` 파일에서 본인이 선택한 단어를 대소문자 구분하지 말고 계수하시오.
> 작업실행결과를 `new >.tsv`  파일에 저장시키시오.
>
> > ## 해답
> > ~~~
> > grep -i hero *a.tsv > new.tsv
> > ~~~
> > {: .bash}
> {: .solution}
{: .challenge}

> ## 선택한 파일에 대소문자 구분 검색 (단어 자체)
> 해당 디렉토리 'America', 'Africa' `.tsv` 파일에서 본인이 선택한 단어 자체만을 대소문자 구분하지 말고 계수하시오.
> 작업실행결과를 `new >.tsv`  파일에 저장시키시오.
>
> > ## 해답
> > ~~~
> > grep -iw hero *a.tsv > new2.tsv
> > ~~~
> > {: .bash}
> {: .solution}
{: .challenge}

생성시킨 마지막에 파일 두개 행수를 비교하시오.

~~~
wc -l FILENAMES
~~~
{: .bash}

텍스트 편집기(Notepad++, Atom, Kate, Sublime Text 등) 혹은 엑셀같은 프로그램에서 파일을 열고,
문자열을 검색하는 것과 `grep`을 사용해서 단어 자체를 검색하는 차이를 살펴본다.

